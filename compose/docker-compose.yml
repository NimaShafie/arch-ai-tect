services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports: ["${OLLAMA_PORT:-11434}:11434"]
    volumes:
      - ollama_models:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 60
      start_period: 120s
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "3" }

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_started
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_NAME=${WEBUI_NAME:-Architecture Workbench}
    ports: ["${OPENWEBUI_PORT:-3000}:8080"]
    volumes:
      - openwebui_data:/app/backend/data
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/ || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 15s
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "3" }

  # MkDocs dev server (compose file lives in ./compose, so mount ../)
  docs:
    image: squidfunk/mkdocs-material:latest
    container_name: docs
    restart: unless-stopped
    working_dir: /work
    command: serve -a 0.0.0.0:8080   # ENTRYPOINT is "mkdocs"
    volumes:
      - ../:/work:ro                 # mount the repo root correctly
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://127.0.0.1:8080', timeout=2)\""]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 15s
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "3" }

  kroki:
    image: yuzutech/kroki
    container_name: kroki
    restart: unless-stopped
    ports: ["${KROKI_PORT:-8000}:8000"]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8000/ || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 10
      start_period: 20s
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "3" }

  plantuml:
    image: plantuml/plantuml-server:jetty
    container_name: plantuml
    restart: unless-stopped
    ports: ["${PLANTUML_PORT:-18080}:8080"]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/ || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 10
      start_period: 20s
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "3" }

  cloudflared:
    image: cloudflare/cloudflared:latest
    container_name: cloudflared
    restart: unless-stopped
    command: tunnel run
    environment:
      - TUNNEL_TOKEN=${CF_TUNNEL_TOKEN}
    depends_on:
      docs:
        condition: service_started
      openwebui:
        condition: service_started
    profiles: ["tunnel"]
    healthcheck:
      test: ["CMD-SHELL", "pgrep -x cloudflared >/dev/null || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 20
      start_period: 10s
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "3" }

volumes:
  ollama_models: {}
  openwebui_data: {}
