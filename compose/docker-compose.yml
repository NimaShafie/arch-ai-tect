services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports: ["${OLLAMA_PORT:-11434}:11434"]
    volumes:
      - ollama_models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_THREADS=8       # = VM vCPU count you set
      - OLLAMA_NUM_PARALLEL=1      # one prompt at a time = lower latency
      - OLLAMA_FLASH_ATTENTION=false
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 120s
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "3" }
    networks: [proxy]

  # One-shot init job to manage models in the shared volume
  ollama-init:
    image: ollama/ollama:latest
    container_name: ollama-init
    depends_on:
      ollama:
        condition: service_started
    volumes:
      - ollama_models:/root/.ollama
    entrypoint: ["/bin/sh","-lc"]
    command: |
      set -e
      echo "Pulling qwen2.5:3b..."
      ollama pull qwen2.5:3b || true
      echo "Removing qwen3:8b (if present)..."
      ollama rm qwen3:8b || true
      echo "Model setup complete."
    restart: "no"
    networks: [proxy]

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_started
      ollama-init:
        condition: service_completed_successfully
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_NAME=ArchAiTect Workbench (Open WebUI)
      - ENABLE_MULTI_USER=true
      - DEFAULT_MODELS=qwen2.5:3b
      - OLLAMA_KEEP_ALIVE=1h
      - ENABLE_REGISTRATION=False
      - ALLOWED_EMAIL_DOMAINS=csun.edu,my.csun.edu,gmail.com
      - ALLOW_EMAIL_LIST=nimzshafie@gmail.com
      - WEBUI_AUTH_TRUSTED_EMAIL_HEADER=CF-Access-Authenticated-User-Email
    ports: ["${OPENWEBUI_PORT:-3001}:8080"]
    volumes:
      - openwebui_data:/app/backend/data
      - openwebui_cache:/root/.cache
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:8080/api/config >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 20
      start_period: 60s
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "3" }
    networks: [proxy]

  docs:
    build: ./docs
    image: local/mkdocs-material-with-plugins:latest
    container_name: docs
    restart: unless-stopped
    working_dir: /work
    command: serve -a 0.0.0.0:8080
    environment:
      - XDG_CACHE_HOME=/var/mkdocs/cache
    volumes:
      - ../:/work:ro
      - docs_site:/var/mkdocs/site
      - docs_cache:/var/mkdocs/cache
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:8080 >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 20s
    networks: [proxy]

  kroki:
    image: yuzutech/kroki:latest
    container_name: kroki
    restart: unless-stopped
    ports: ["${KROKI_PORT:-8000}:8000"]
    environment:
      - KROKI_MAX_URI_LENGTH=16384
      - KROKI_MAX_CONTENT_LENGTH=1048576
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8000/health || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 10
      start_period: 20s
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "3" }
    networks: [proxy]

  plantuml:
    image: plantuml/plantuml-server:jetty
    container_name: plantuml
    restart: unless-stopped
    ports: ["${PLANTUML_PORT:-18080}:8080"]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/ || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 10
      start_period: 20s
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "3" }
    networks: [proxy]

  webhook:
    image: node:20-alpine
    container_name: webhook
    working_dir: /opt/auto-deploy
    volumes:
      - /opt/auto-deploy:/opt/auto-deploy:ro
    command: ["node", "webhook.js"]
    environment:
      - PORT=9000
      - WEBHOOK_SECRET=${WEBHOOK_SECRET}
    depends_on:
      cloudflared:
        condition: service_started
    # no "ports:" section â€” not exposed on the host

  cloudflared:
    image: cloudflare/cloudflared:latest
    container_name: cloudflared
    restart: unless-stopped
    command: tunnel run
    environment:
      - TUNNEL_TOKEN=${CF_TUNNEL_TOKEN}
    depends_on:
      docs:
        condition: service_started
      openwebui:
        condition: service_started
      kroki:
        condition: service_started
      plantuml:
        condition: service_started
    profiles: ["tunnel"]
    healthcheck:
      test: ["CMD-SHELL", "pgrep -x cloudflared >/dev/null || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 20
      start_period: 10s
    networks: [proxy]

volumes:
  ollama_models: {}
  openwebui_data: {}
  openwebui_cache: {}
  docs_site: {}
  docs_cache: {}

networks:
  proxy:
    external: true
